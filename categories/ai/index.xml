<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ai on SeeYu</title><link>https://www.notes.wang/categories/ai/</link><description>Recent content in ai on SeeYu</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 19 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.notes.wang/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>AI发展史</title><link>https://www.notes.wang/post/ai%E5%8F%91%E5%B1%95%E5%8F%B2/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://www.notes.wang/post/ai%E5%8F%91%E5%B1%95%E5%8F%B2/</guid><description>
人工智能发展史：从图灵测试到GPT时代的历程 当你在深夜与ChatGPT畅聊人生，当Midjourney为你画出想象中的奇幻场景，当Sora生成逼真的视频片段——你是否想过，这一切是如何开始的？
人工智能（AI）的发展史，是一部人类探索智能奥秘、挑战自我极限的壮丽史诗。今天，让我们穿越时光，回顾这段充满激情、挫折与奇迹的旅程。
萌芽期：梦想的种子（1950年代之前） “机器能思考吗？”
1936年，天才数学家艾伦·图灵提出了“图灵机”的概念，为计算机科学奠定了基础。1950年，他发表了划时代的论文《计算机器与智能》，开篇就提出了这个问题，并设计了著名的“图灵测试”——如果一台机器能在对话中让人无法分辨它是人还是机器，就说明它具有智能。
与此同时，神经科学家也在探索大脑的工作原理。1943年，麦卡洛克和皮茨提出了第一个神经元的数学模型，为神经网络埋下了种子。
诞生：达特茅斯会议的夏天（1956年） 1956年夏天，美国达特茅斯学院的一场会议，正式宣告了人工智能的诞生。
约翰·麦卡锡（他创造了“Artificial Intelligence”这个词）、马文·明斯基、克劳德·香农等十位科学家聚在一起，畅想：“如果能精确描述学习或智能的每一个方面，就能制造一台机器来模拟它。”
这个夏天点燃了第一波AI热潮。早期成果令人振奋：
1951年，第一个神经网络SNARC问世 1956年，逻辑理论家程序证明了数学定理 1957年，弗兰克·罗森布拉特发明了感知机 寒冬与复苏：期望与现实的落差（1970s-1980s） 第一次AI寒冬（1974-1980）
早期的乐观很快被现实浇醒。当时的计算机算力不足，感知机甚至无法解决简单的异或问题。1969年，明斯基和帕佩特出版《感知机》一书，指出了神经网络的局限性。加上英国莱特希尔报告对AI的严厉批评，英美政府大幅削减经费，AI进入第一个寒冬。
专家系统时代（1980s）
1980年代，一种新思路带来转机——专家系统。通过将人类专家的知识编码成规则，程序可以在特定领域（如医疗诊断、地质勘探）提供专业建议。
日本1981年启动第五代计算机项目，欧美也纷纷跟进。但专家系统难以获取知识、难以应对复杂现实，维护成本高昂。1980年代末，随着日本项目失败，AI迎来第二个寒冬。
第三次浪潮：机器学习的崛起（1990s-2010） 这一次，AI不再试图模仿人类推理，而是转向数据驱动的方法。
关键突破：
1989年，杨立昆（Yann LeCun）提出卷积神经网络，用于手写数字识别 1997年，IBM的深蓝击败国际象棋冠军卡斯帕罗夫 支持向量机、随机森林等算法成熟 统计革命：AI从逻辑推理转向概率统计，从“告诉我规则”变成“从数据中学习”。
深度学习革命（2012-2018） 2012年，ImageNet图像识别大赛上，辛顿团队用深度神经网络AlexNet将错误率从25%降至15%，震惊学界。
这背后是三大要素的完美结合：
大数据：互联网带来了海量数据 强算力：GPU让大规模并行计算成为可能 新算法：ReLU激活函数、Dropout等技术 此后，AI迎来爆发：
2016年，AlphaGo击败李世石 2017年，Transformer架构论文《Attention Is All You Need》发表 计算机视觉、语音识别达到人类水平 大语言模型时代（2018-至今） Transformer架构开启了大语言模型时代：
2018年：GPT-1发布，参数量1.17亿 2019年：GPT-2（15亿参数），因“太危险”暂不开放 2020年：GPT-3（1750亿参数），展现惊人能力 2022年：ChatGPT发布，5天用户破百万 2023-2024年：GPT-4、Claude、Gemini、Sora等涌现 我们正处在AI发展最快的历史时期。
结语：历史的启示 回顾AI发展史，可以总结出几个规律：
周期律：狂热 → 失望 → 寒冬 → 新突破 → 再狂热 技术融合：每次突破都源于算力、数据、算法的协同进化 基础研究的重要性：Transformer架构源于2017年的论文 面对AI的未来，保持理性乐观最为重要。1956年达特茅斯会议上的先驱们，如果看到今天的AI，一定会感到欣慰。而今天的我们，正站在一个新的历史起点上。
未来，更加深入的AI探索，以及将AI运用到各个行业，势必会多点开花。</description></item></channel></rss>